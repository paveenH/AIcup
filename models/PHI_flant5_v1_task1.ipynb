{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN9rPwuNs4RRwUwSc/fqFnM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Environment"],"metadata":{"id":"Xa69w_Cu8EqH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vo1u9HiTXAFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install transformers\n","!pip install datasets\n","!pip install sentencepiece"],"metadata":{"id":"T5iC4JQJhkpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","import io\n","import json\n","import math\n","import os\n","import time\n","import re\n","import random\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset, Features, Value, concatenate_datasets, Dataset\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.optim import lr_scheduler, AdamW\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm, trange\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from transformers import get_linear_schedule_with_warmup"],"metadata":{"id":"SIj12rVkM-ZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Constant"],"metadata":{"id":"FC23OQgTp5OW"}},{"cell_type":"code","source":["\"\"\"Basic Setting\"\"\"\n","data_path = \"/content/drive/MyDrive/AIcup/data_hpw\"\n","model_path = \"/content/drive/MyDrive/AIcup/model_hpw\"\n","\n","\"\"\"Data Setting\"\"\"\n","TaskPrefixOriginal = \"Private information extraction from patients' records: \"\n","TaskPrefixSliced = \"Private information extraction from sliced patients' records: \"\n","TaskPrefixSpliced = \"Private information extraction from spliced patients' records: \"\n","PHINull = \"PHI:Null\"\n","IgnoredPadIdx = -100\n","PHINullRatio = 0.3\n","PhiCategory = ['PATIENT', 'DOCTOR', 'USERNAME', 'PROFESSION',\n","                'ROOM', 'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION',\n","                'STREET', 'CITY', 'STATE', 'COUNTRY', 'ZIP', 'LOCATION-OTHER',\n","                'AGE', 'DATE', 'TIME', 'DURATION', 'SET', 'PHONE', 'FAX', 'EMAIL',\n","                'URL', 'IPADDR', 'SSN', 'MEDICALRECORD', 'HEALTHPLAN', 'ACCOUNT', 'LICENSE',\n","                'VEHICLE', 'DEVICE', 'BIOID', 'IDNUM']\n","\"\"\"Model Constant Setting\"\"\"\n","BatchSize = 16\n","Epochs = int(BatchSize*0.75)\n","MaxLen = 128\n","LearningRate = 1e-4\n","WeightDecay = 0.05\n","PretrainedModel = \"google/flan-t5-large\""],"metadata":{"id":"yQ1MuI-Vp9kw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataReading"],"metadata":{"id":"DC7ThvQkeW-B"}},{"cell_type":"code","source":["\"\"\"Filter PHINull\"\"\"\n","def filter_phi_null_ratio(dataset, ratio):\n","    # get indices\n","    phi_null_indices = [i for i, item in enumerate(dataset) if item['label'] == 'PHI:Null']\n","    other_indices = [i for i, item in enumerate(dataset) if item['label'] != 'PHI:Null']\n","    print(f\"Total {len(dataset)}, PHI null {len(phi_null_indices)}, others {len(other_indices)} \")\n","\n","    # target null count\n","    target_phi_null_count = int(min(len(phi_null_indices), len(other_indices) * ratio))\n","    print(\"target_phi_null_count\", target_phi_null_count)\n","\n","    if len(phi_null_indices) > target_phi_null_count:\n","        phi_null_indices = random.sample(phi_null_indices, target_phi_null_count)\n","\n","    # reunion indices\n","    final_indices = phi_null_indices + other_indices\n","    random.shuffle(final_indices)\n","\n","    final_dataset = dataset.select(final_indices)\n","    return final_dataset"],"metadata":{"id":"OBGl1rlD8q_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Add Prefix\"\"\"\n","def add_prefix_to_sample(sample, prefix):\n","    sample['content'] = prefix + sample['content']\n","    return sample\n","\n","def add_prefix_to_dataset(dataset, prefix):\n","    # 使用 map 函数为每个样本添加前缀\n","    return dataset.map(lambda x: add_prefix_to_sample(x, prefix))"],"metadata":{"id":"LQmBNYqmwcbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_reading(data_path1, data_path2, task_prefix):\n","  # read data\n","  data1 = load_dataset(\"csv\", data_files=data_path1, delimiter='\\t',\n","                       features = Features({\n","                          'fid': Value('string'), 'idx': Value('int64'),\n","                          'content': Value('string'), 'label': Value('string')}),\n","                       column_names=['fid', 'idx', 'content', 'label'], keep_default_na=False)[\"train\"]\n","  data2 = load_dataset(\"csv\", data_files=data_path2, delimiter='\\t',\n","                       features = Features({\n","                          'fid': Value('string'), 'idx': Value('int64'),\n","                          'content': Value('string'), 'label': Value('string')}),\n","                       column_names=['fid', 'idx', 'content', 'label'], keep_default_na=False)[\"train\"]\n","  # add task prefix\n","  data1 = add_prefix_to_dataset(data1, task_prefix)\n","  data2 = add_prefix_to_dataset(data2, task_prefix)\n","\n","  print(\"pahse1 data:\", len(data1))\n","  print(\"pahse2 data:\", len(data2))\n","  print(\"data sample:\", data1[200])\n","\n","  # concatenate data\n","  data = concatenate_datasets([data1, data2])\n","  print(\"data length after cancatenation:\", len(data))\n","\n","  # filter data\n","  filtered_data = filter_phi_null_ratio(data, PHINullRatio)\n","  # print filter information\n","  phi_null_count_after = len([item for item in filtered_data if item['label'] == 'PHI:Null'])\n","  other_count_after = len(filtered_data) - phi_null_count_after\n","  phi_null_ratio = phi_null_count_after / other_count_after\n","  print(\"After filtering:\")\n","  print(\"Length after filter:\", len(filtered_data))\n","  print(\"PHI: NULL count:\", phi_null_count_after)\n","  print(\"Other labels count:\", other_count_after)\n","  print(\"PHI: NULL to Other labels ratio:\", phi_null_ratio)\n","  return filtered_data"],"metadata":{"id":"u3Ofc_KVtvHT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Original data paths\n","data_org_train_path1 = os.path.join(data_path, \"train_phase1_v8_original_train.tsv\")\n","data_org_train_path2 = os.path.join(data_path, \"train_phase2_v8_original_train.tsv\")\n","data_org_test_path1 = os.path.join(data_path, \"train_phase1_v8_original_test.tsv\")\n","data_org_test_path2 = os.path.join(data_path, \"train_phase2_v8_original_test.tsv\")\n","\n","# Sliced data paths\n","data_slc_train_path1 = os.path.join(data_path, \"train_phase1_v8_sliced_train.tsv\")\n","data_slc_train_path2 = os.path.join(data_path, \"train_phase2_v8_sliced_train.tsv\")\n","data_slc_test_path1 = os.path.join(data_path, \"train_phase1_v8_sliced_test.tsv\")\n","data_slc_test_path2 = os.path.join(data_path, \"train_phase2_v8_sliced_test.tsv\")\n","\n","# Spliced data paths\n","data_spl_train_path1 = os.path.join(data_path, \"train_phase1_v8_spliced_train.tsv\")\n","data_spl_train_path2 = os.path.join(data_path, \"train_phase2_v8_spliced_train.tsv\")\n","data_spl_test_path1 = os.path.join(data_path, \"train_phase1_v8_spliced_test.tsv\")\n","data_spl_test_path2 = os.path.join(data_path, \"train_phase2_v8_spliced_test.tsv\")\n","\n","# Read datasets\n","data_org_train = data_reading(data_org_train_path1, data_org_train_path2, TaskPrefixOriginal)\n","data_org_test = data_reading(data_org_test_path1, data_org_test_path2, TaskPrefixOriginal)\n","\n","data_slc_train = data_reading(data_slc_train_path1, data_slc_train_path2, TaskPrefixSliced)\n","data_slc_test = data_reading(data_slc_test_path1, data_slc_test_path2, TaskPrefixSliced)\n","\n","data_spl_train = data_reading(data_spl_train_path1, data_spl_train_path2, TaskPrefixSpliced)\n","data_spl_test = data_reading(data_spl_test_path1, data_spl_test_path2, TaskPrefixSpliced)"],"metadata":{"id":"cSoJgDuVvCfU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = concatenate_datasets([data_org_train, data_slc_train, data_spl_train])\n","print(\"total train data:\", len(train_data))\n","train_data = train_data.shuffle(seed=42)\n","# train_data = train_data.select(range(30000))\n","# print(\"sampled train data:\", len(train_data))\n","for i in range(100, 103):\n","  print(\"train data samples:\", train_data[i])\n","\n","test_data = concatenate_datasets([data_org_test, data_slc_test, data_spl_test])\n","test_data = test_data.shuffle(seed=42)\n","print(\"total test data:\", len(test_data))\n","# test_data = test_data.select(range(3000))\n","# print(\"sampled test data:\", len(test_data))\n","for i in range(100, 103):\n","  print(\"test data samples:\", test_data[i])"],"metadata":{"id":"hyZwI5X7eW-D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ModelConfig"],"metadata":{"id":"krJBkubBhuAR"}},{"cell_type":"code","source":["\"\"\"Model Config\"\"\"\n","tokenizer = T5Tokenizer.from_pretrained(PretrainedModel)\n","\n","model = T5ForConditionalGeneration.from_pretrained(PretrainedModel)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"sshXIakGiQHu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataloaderTrain"],"metadata":{"id":"wrozFi3Msk4w"}},{"cell_type":"code","source":["\"\"\"Tokenizer Template\"\"\"\n","def collate_batch_with_prompt_template(batch, tokenizer, IGNORED_PAD_IDX=IgnoredPadIdx):\n","    texts = [data['content'] for data in batch]\n","    encoded_seq = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=MaxLen, return_tensors=\"pt\")\n","\n","    labels = [data['label'] for data in batch]\n","    encoded_label = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=MaxLen, return_tensors=\"pt\")['input_ids']\n","    encoded_label[encoded_label == tokenizer.pad_token_id] = IGNORED_PAD_IDX\n","\n","    return encoded_seq['input_ids'], encoded_seq['attention_mask'], encoded_label"],"metadata":{"id":"ZsMQg0dZe71Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Dataloader\"\"\"\n","# train dataloader\n","train_dataloader = DataLoader(train_data,\n","                              batch_size=BatchSize,\n","                              collate_fn=lambda batch: collate_batch_with_prompt_template(batch, tokenizer),\n","                              shuffle=True,\n","                              drop_last=True)\n","# dev dataloader\n","test_dataloader = DataLoader(test_data,\n","                             batch_size=1,\n","                             collate_fn=lambda batch: collate_batch_with_prompt_template(batch, tokenizer),\n","                             shuffle=False,\n","                             drop_last=True)\n","\n","dataloaders = {\"train\": train_dataloader, \"test\": test_dataloader}\n","print(len(train_dataloader))\n","print(len(test_dataloader))"],"metadata":{"id":"57XTXhV4F5jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Test Train_dataloader\"\"\"\n","titer = iter(train_dataloader)\n","tks, masks, labels = next(titer)\n","print(tks.shape)\n","print(tks[0])\n","print()\n","print(masks.shape)\n","print(masks[0])\n","print()\n","print(labels.shape)\n","print(labels[0])\n","print()"],"metadata":{"id":"DaKF8OxeF5jc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optimizer"],"metadata":{"id":"FtWWC5TssflF"}},{"cell_type":"code","source":["\"\"\"optimizer config\"\"\"\n","optimizer = AdamW(model.parameters(), lr=LearningRate, weight_decay=WeightDecay)\n","\n","# steps calculation\n","num_training_steps = len(dataloaders[\"train\"])*Epochs\n","CountSteps = int(num_training_steps*0.1/Epochs) // 10 *10*(16//BatchSize)\n","WarmUpSteps = int(num_training_steps*0.01)\n","print(f\"num_training_steps {num_training_steps}, warn_up_steps {WarmUpSteps}, count each {CountSteps} steps\")\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=WarmUpSteps,\n","    num_training_steps=num_training_steps\n",")"],"metadata":{"id":"6FUbEDowsUdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"J70HZYqcKWd7"}},{"cell_type":"code","source":["def write_losses(loss_path, train_losses, dev_losses):\n","  losses={}\n","  losses[\"train\"] = train_losses\n","  losses[\"test\"] = dev_losses\n","  with open(loss_path, \"w\") as out_config:\n","    json.dump(losses, out_config, indent=4)"],"metadata":{"id":"IHDRj3hFUTU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Train Model\"\"\"\n","# model path\n","name = str(int(time.time()))\n","save_path = os.path.join(model_path, name)\n","model_name = save_path + f\"/best_{name}.pt\"\n","\n","loss_path = os.path.join(save_path, \"loss.csv\")\n","\n","if not os.path.isdir(save_path):\n","    os.mkdir(save_path)\n","\n","best_loss = float('inf')\n","train_losses = []\n","test_losses = []\n","\n","for epoch in range(Epochs):\n","    print(\"[Training] Epoch {}/{}\".format(epoch, Epochs - 1))\n","    print(\"-\" * 10)\n","\n","    running_loss_train = 0.0\n","    running_loss_test = 0.0\n","    intermediate_loss = 0.0\n","\n","    # Training phase\n","    model.train()\n","    for count, (seqs, masks, labels) in enumerate(tqdm(dataloaders[\"train\"])):\n","        seqs, labels, masks = seqs.to(device), labels.to(device), masks.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids=seqs, labels=labels, attention_mask=masks)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","        running_loss_train += loss.item()\n","        intermediate_loss += loss.item()\n","        # Print training losses\n","        if count % CountSteps == 0 and count != 0:\n","            average_loss = intermediate_loss / CountSteps\n","            print(f\"Loss in epoch{epoch}-step{count}: {average_loss:.4f}\")\n","            train_losses.append(average_loss)\n","            intermediate_loss = 0\n","\n","    # Print epoch train losses\n","    epoch_train_loss = running_loss_train / len(dataloaders[\"train\"])\n","    print(f\"[Training] Train Loss: {epoch_train_loss:.4f}\")\n","\n","    # Testing phase\n","    model.eval()\n","    with torch.no_grad():\n","        for seqs_test, masks_test, labels_test in tqdm(dataloaders[\"test\"]):\n","            seqs_test, labels_test, masks_test = seqs_test.to(device), labels_test.to(device), masks_test.to(device)\n","            outputs = model(input_ids=seqs_test, labels=labels_test, attention_mask=masks_test)\n","            # loss = outputs.loss.mean()\n","            loss = outputs.loss\n","            running_loss_test += loss.item()\n","\n","    # print epoch test loss\n","    epoch_test_loss = running_loss_test / len(dataloaders[\"test\"])\n","    print(f\"[Training] Test Loss: {epoch_test_loss:.4f}\")\n","\n","    # save the best model\n","    test_losses.append(epoch_test_loss)\n","\n","    if epoch_test_loss < best_loss:\n","        # write losses in each epoch\n","        write_losses(loss_path, train_losses, test_losses)\n","        best_loss = epoch_test_loss\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        torch.save(model.state_dict(), model_name)\n","        print(f\"[INFO] Updated best model on dev checkpoint: {model_name}\")"],"metadata":{"id":"mlph_1KqLUo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataloaderVal"],"metadata":{"id":"aksom6hSBc71"}},{"cell_type":"code","source":["\"\"\"Add Prefix\"\"\"\n","def add_prefix_to_sample(sample, prefix):\n","    sample['content'] = prefix + sample['content']\n","    return sample\n","\n","def add_prefix_to_dataset(dataset, prefix):\n","    return dataset.map(lambda x: add_prefix_to_sample(x, prefix))"],"metadata":{"id":"ci9fsaEtzm-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reading_validation_data(val_path, task_prefix):\n","  # read data\n","  val_data = load_dataset(\"csv\", data_files=val_path, delimiter='\\t',\n","                        features = Features({\n","                            'fid': Value('string'), 'idx': Value('int64'),\n","                            'content': Value('string'), 'label': Value('string')}),\n","                        column_names=['fid', 'idx', 'content', 'label'])[\"train\"]\n","\n","  # add prefix to each dataset\n","  val_data = add_prefix_to_dataset(val_data, task_prefix)\n","  print(len(val_data))\n","  print(\"data sample:\", val_data[20])\n","  return val_data"],"metadata":{"id":"9ZQzTiGvxfez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Tokenizer Template\"\"\"\n","def collate_batch_with_prompt_template_val(batch, tokenizer, ):\n","    texts = [data['content'] for data in batch]\n","    fids = [data['fid'] for data in batch]\n","    pos = [data['idx'] for data in batch]\n","    contents = [data['content'] for data in batch]\n","    encoded_seq = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=MaxLen, return_tensors=\"pt\")\n","    return encoded_seq['input_ids'], encoded_seq['attention_mask'], fids, pos, contents"],"metadata":{"id":"5oS9VHDwBc72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Get validation datasets\"\"\"\n","val_path_org = os.path.join(data_path, \"valid_phase1_v8_original.tsv\")\n","val_path_slc = os.path.join(data_path, \"valid_phase1_v8_sliced.tsv\")\n","val_path_spl = os.path.join(data_path, \"valid_phase1_v8_spliced.tsv\")\n","\n","val_data_org = reading_validation_data(val_path_org, TaskPrefixOriginal)\n","val_data_slc = reading_validation_data(val_path_slc, TaskPrefixSliced)\n","val_data_spl = reading_validation_data(val_path_spl, TaskPrefixSpliced)\n","\n","# Concatenate datasets\n","val_data = concatenate_datasets([val_data_org, val_data_slc, val_data_spl])\n","val_data = list(val_data)\n","# val_data = list(val_data)[:100]\n","print(\"validation length:\", len(val_data))\n","for i in range(100, 103):\n","  print(val_data[i])"],"metadata":{"id":"Qi4nw6eaBc73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ValBatchSize = 128\n","val_dataloader = DataLoader(val_data,\n","                            batch_size=ValBatchSize,\n","                            collate_fn=lambda batch: collate_batch_with_prompt_template_val(batch, tokenizer),\n","                            shuffle=False)"],"metadata":{"id":"GQ7wVa-kBc73"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generation\n"],"metadata":{"id":"W_4L7n36s7Gf"}},{"cell_type":"code","source":["\"\"\"model reload\"\"\"\n","name = \"1701317300_flant5_v1_large_task1\"\n","time = name[:10]\n","model_name = f\"drive/MyDrive/AIcup/model_hpw/{name}/best_{time}.pt\"\n","answer_path = f\"drive/MyDrive/AIcup/model_hpw/{name}/answer_{time}.txt\"\n","prediction_path = f\"drive/MyDrive/AIcup/model_hpw/{name}/prediction_{time}.txt\"\n","model.load_state_dict(torch.load(model_name))\n","model = model.to(device)"],"metadata":{"id":"lnqc8gvssB77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_prefixes(text):\n","    if TaskPrefixOriginal in text:\n","        text = text.replace(TaskPrefixOriginal, \"\")\n","    if TaskPrefixSliced in text:\n","        text = text.replace(TaskPrefixSliced, \"\")\n","    if TaskPrefixSpliced in text:\n","        text = text.replace(TaskPrefixSpliced, \"\")\n","    return text"],"metadata":{"id":"ej8Ejt9oMG_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(os.path.join(prediction_path), 'w', encoding='utf8') as f_predictions:\n","  for seqs, masks, fids, pos, contents in tqdm(val_dataloader):\n","    contents = list(map(remove_prefixes, contents))\n","    with torch.no_grad():\n","      seqs, masks = seqs.to(device), masks.to(device)\n","      predicted_tokens = model.generate(input_ids=seqs, attention_mask=masks)\n","      predicted_tokens = torch.squeeze(predicted_tokens)\n","      predicted_strings = tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True)\n","      for idx, pred in enumerate(predicted_strings):\n","        if PHINull in pred.strip():\n","          continue\n","        # print(f'{fids[idx]}\\t{pos[idx]}\\t{contents[idx]}\\t{pred}')\n","        f_predictions.write(f'{fids[idx]}\\t{pos[idx]}\\t{contents[idx]}\\t{pred}\\n')"],"metadata":{"id":"LLKERJEZn271"},"execution_count":null,"outputs":[]}]}