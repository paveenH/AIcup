{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1xUVwjM6tLsEwxyZLkG6iskMSYAjnF11C","timestamp":1701152876162}],"gpuType":"T4","collapsed_sections":["Xa69w_Cu8EqH","FC23OQgTp5OW","o59l-BaOuuM9","krJBkubBhuAR","wrozFi3Msk4w","FtWWC5TssflF","J70HZYqcKWd7"],"authorship_tag":"ABX9TyNp3o7kh1nAZ2YvvnFdh4bp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Environment"],"metadata":{"id":"Xa69w_Cu8EqH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vo1u9HiTXAFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install transformers\n","!pip install datasets\n","!pip install sentencepiece\n","!pip install peft\n","!pip install accelerate"],"metadata":{"id":"TUUQEkORoLIN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","import io\n","import json\n","import math\n","import os\n","import time\n","import re\n","import random\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset, Features, Value, concatenate_datasets, Dataset\n","from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType, PeftModel\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.optim import lr_scheduler, AdamW\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm, trange\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from transformers import get_linear_schedule_with_warmup"],"metadata":{"id":"bVQ6jDwaoLIO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Constant"],"metadata":{"id":"FC23OQgTp5OW"}},{"cell_type":"code","source":["\"\"\"Basic Setting\"\"\"\n","data_path = \"/content/drive/MyDrive/AIcup/data_hpw\"\n","model_path = \"/content/drive/MyDrive/AIcup/model_hpw\"\n","\n","\"\"\"Data Setting\"\"\"\n","TaskPrefix = \"Time normalization: \"\n","PHINull = \"PHI:Null\"\n","# TaskPrefix = \"\"\n","IgnoredPadIdx = -100\n","PhiCategory = ['PATIENT', 'DOCTOR', 'USERNAME', 'PROFESSION',\n","                'ROOM', 'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION',\n","                'STREET', 'CITY', 'STATE', 'COUNTRY', 'ZIP', 'LOCATION-OTHER',\n","                'AGE', 'DATE', 'TIME', 'DURATION', 'SET', 'PHONE', 'FAX', 'EMAIL',\n","                'URL', 'IPADDR', 'SSN', 'MEDICALRECORD', 'HEALTHPLAN', 'ACCOUNT', 'LICENSE',\n","                'VEHICLE', 'DEVICE', 'BIOID', 'IDNUM']\n","NormCategory = ['DATE', 'TIME', 'DURATION', 'SET']\n","\"\"\"Model Constant Setting\"\"\"\n","BatchSize = 64\n","Epochs = int(BatchSize*0.75)\n","MaxLen = 32\n","LearningRate = 3e-5\n","WeightDecay = 0.01\n","PretrainedModel = \"google/flan-t5-base\""],"metadata":{"id":"yQ1MuI-Vp9kw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DatasetTrain"],"metadata":{"id":"o59l-BaOuuM9"}},{"cell_type":"code","source":["\"\"\"Data Reading\"\"\"\n","data_path1 = os.path.join(data_path, \"train_phase1_v3_task2.tsv\")\n","data_path2 = os.path.join(data_path, \"train_phase2_v3_task2.tsv\")\n","\n","data1 = load_dataset(\"csv\", data_files=data_path1, delimiter='\\t',\n","                     features = Features({\n","                        'content': Value('string'), 'label': Value('string')}),\n","                     column_names=['content', 'label'], keep_default_na=False)\n","\n","data2 = load_dataset(\"csv\", data_files=data_path2, delimiter='\\t',\n","                     features = Features({\n","                        'content': Value('string'), 'label': Value('string')}),\n","                     column_names=['content', 'label'], keep_default_na=False)\n","\n","print(\"pahse1 data:\", len(data1[\"train\"]))\n","print(\"pahse2 data:\", len(data2[\"train\"]))\n","print()\n","print(\"data sample:\", data1[\"train\"][200])\n","print(type(data1[\"train\"]))"],"metadata":{"id":"KjqFzLAVYrUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Data Concatenation\"\"\"\n","data = concatenate_datasets([data1['train'], data2['train']])\n","print(\"data length:\", len(data))\n","print(data[1025])\n","\n","# \"\"\"small scale for test\"\"\"\n","# data = data.select(range(90000))\n","# print(\"test data length:\", len(data))"],"metadata":{"id":"ob2x4Ms8GCe2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = data.train_test_split(test_size=0.10, seed=25).values()\n","print(\"train data size:\", len(train_data))\n","print(\"test data size:\", len(test_data))"],"metadata":{"id":"Wpn1OlBc8k9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ModelConfig"],"metadata":{"id":"krJBkubBhuAR"}},{"cell_type":"code","source":["\"\"\"Model Config\"\"\"\n","# tokenizer\n","tokenizer = T5Tokenizer.from_pretrained(PretrainedModel)\n","\n","# model config\n","model = T5ForConditionalGeneration.from_pretrained(PretrainedModel)\n","# model = T5ForConditionalGeneration.from_pretrained(PretrainedModel, load_in_8bit=True)\n","\n","# LoRA config\n","lora_config = LoraConfig(\n"," r=16,\n"," lora_alpha=32,\n"," target_modules=[\"q\", \"v\"],\n"," lora_dropout=0.05,\n"," bias=\"none\",\n"," task_type=TaskType.SEQ_2_SEQ_LM\n",")\n","\n","# prepare int-8 model for training\n","model = prepare_model_for_int8_training(model)\n","\n","# add LoRA adaptor\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"Q_6yaYc0oFZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataloaderTrain"],"metadata":{"id":"wrozFi3Msk4w"}},{"cell_type":"code","source":["\"\"\"Tokenizer Template\"\"\"\n","def collate_batch_with_prompt_template(batch, tokenizer, IGNORED_PAD_IDX=IgnoredPadIdx):\n","    texts = [TaskPrefix + data['content'] for data in batch]\n","    encoded_seq = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=MaxLen, return_tensors=\"pt\")\n","\n","    labels = [data['label'] for data in batch]\n","    encoded_label = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=MaxLen, return_tensors=\"pt\")['input_ids']\n","    encoded_label[encoded_label == tokenizer.pad_token_id] = IGNORED_PAD_IDX\n","\n","    return encoded_seq['input_ids'], encoded_seq['attention_mask'], encoded_label"],"metadata":{"id":"ixNuBGIqF5jM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Dataloader\"\"\"\n","# train dataloader\n","train_dataloader = DataLoader(train_data,\n","                              batch_size=BatchSize,\n","                              collate_fn=lambda batch: collate_batch_with_prompt_template(batch, tokenizer),\n","                              shuffle=True,\n","                              drop_last=True)\n","# dev dataloader\n","test_dataloader = DataLoader(test_data,\n","                             batch_size=1,\n","                             collate_fn=lambda batch: collate_batch_with_prompt_template(batch, tokenizer),\n","                             shuffle=False,\n","                             drop_last=True)\n","\n","dataloaders = {\"train\": train_dataloader, \"test\": test_dataloader}\n","print(len(train_dataloader))\n","print(len(test_dataloader))"],"metadata":{"id":"57XTXhV4F5jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Test Train_dataloader\"\"\"\n","titer = iter(train_dataloader)\n","tks, masks, labels = next(titer)\n","print(tks.shape)\n","print(tks[0])\n","print()\n","print(masks.shape)\n","print(masks[0])\n","print()\n","print(labels.shape)\n","print(labels[0])\n","print()"],"metadata":{"id":"DaKF8OxeF5jc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optimizer"],"metadata":{"id":"FtWWC5TssflF"}},{"cell_type":"code","source":["\"\"\"optimizer config\"\"\"\n","optimizer = AdamW(model.parameters(), lr=LearningRate, weight_decay=WeightDecay)\n","\n","# steps calculation\n","num_training_steps = len(dataloaders[\"train\"])*Epochs\n","CountSteps = int(num_training_steps*0.1/Epochs) // 10 *10\n","if CountSteps == 0:\n","    CountSteps = 10\n","# warm up\n","WarmUpSteps = int(num_training_steps*0.01)\n","print(f\"num_training_steps {num_training_steps}, warn_up_steps {WarmUpSteps}, count each {CountSteps} steps\")\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=WarmUpSteps,\n","    num_training_steps=num_training_steps\n",")"],"metadata":{"id":"6FUbEDowsUdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"J70HZYqcKWd7"}},{"cell_type":"code","source":["def write_losses(loss_path, train_losses, dev_losses):\n","  losses={}\n","  losses[\"train\"] = train_losses\n","  losses[\"test\"] = dev_losses\n","  with open(loss_path, \"w\") as out_config:\n","    json.dump(losses, out_config, indent=4)"],"metadata":{"id":"IHDRj3hFUTU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Train Model\"\"\"\n","# model path\n","name = str(int(time.time()))\n","save_path = os.path.join(model_path, name)\n","model_name = save_path + f\"/best_{name}.pt\"\n","\n","loss_path = os.path.join(save_path, \"loss.csv\")\n","\n","if not os.path.isdir(save_path):\n","    os.mkdir(save_path)\n","\n","best_loss = float('inf')\n","train_losses = []\n","test_losses = []\n","\n","for epoch in range(Epochs):\n","    print(\"[Training] Epoch {}/{}\".format(epoch, Epochs - 1))\n","    print(\"-\" * 10)\n","\n","    running_loss_train = 0.0\n","    running_loss_test = 0.0\n","    intermediate_loss = 0.0\n","\n","    # Training phase\n","    model.train()\n","    for count, (seqs, masks, labels) in enumerate(tqdm(dataloaders[\"train\"])):\n","        seqs, labels, masks = seqs.to(device), labels.to(device), masks.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids=seqs, labels=labels, attention_mask=masks)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","        running_loss_train += loss.item()\n","        intermediate_loss += loss.item()\n","        # Print training losses\n","        if count % CountSteps == 0 and count != 0:\n","            average_loss = intermediate_loss / CountSteps\n","            print(f\"Loss in epoch{epoch}-step{count}: {average_loss:.4f}\")\n","            train_losses.append(average_loss)\n","            intermediate_loss = 0\n","\n","    # Print epoch train losses\n","    epoch_train_loss = running_loss_train / len(dataloaders[\"train\"])\n","    print(f\"[Training] Train Loss: {epoch_train_loss:.4f}\")\n","\n","    # Testing phase\n","    model.eval()\n","    with torch.no_grad():\n","        for seqs_test, masks_test, labels_test in tqdm(dataloaders[\"test\"]):\n","            seqs_test, labels_test, masks_test = seqs_test.to(device), labels_test.to(device), masks_test.to(device)\n","            outputs = model(input_ids=seqs_test, labels=labels_test, attention_mask=masks_test)\n","            # loss = outputs.loss.mean()\n","            loss = outputs.loss\n","            running_loss_test += loss.item()\n","\n","    # print epoch test loss\n","    epoch_test_loss = running_loss_test / len(dataloaders[\"test\"])\n","    print(f\"[Training] Test Loss: {epoch_test_loss:.4f}\")\n","\n","    # save the best model\n","    test_losses.append(epoch_test_loss)\n","\n","\n","    if epoch_test_loss < best_loss:\n","        # write losses in each epoch\n","        write_losses(loss_path, train_losses, test_losses)\n","\n","        best_loss = epoch_test_loss\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        # torch.save(model.state_dict(), model_name)\n","        torch.save(model.state_dict(), model_name)\n","        print(f\"[INFO] Updated best model on dev checkpoint: {model_name}\")"],"metadata":{"id":"mlph_1KqLUo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataloaderVal"],"metadata":{"id":"Y-tXKMEkgYqP"}},{"cell_type":"code","source":["\"\"\"get validation dataset\"\"\"\n","answer_name = \"answer_1701317300_1701181213.txt\"\n","val_path = os.path.join(data_path,answer_name )\n","val_data = load_dataset(\"csv\", data_files=val_path, delimiter='\\t',\n","                        features=Features({\n","                            'fid': Value('string'),\n","                            'cat': Value('string'),\n","                            'start_pos': Value('int16'),\n","                            'end_pos': Value('int16'),\n","                            'content': Value('string')\n","                        }),\n","                        column_names=['fid', 'cat', 'start_pos', 'end_pos', 'content'])[\"train\"]\n","\n","val_data= list(val_data)\n","# val_data = val_data[:100]\n","print(val_data)"],"metadata":{"id":"73qiApEMgBBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Tokenizer Template\"\"\"\n","def collate_batch_with_prompt_template_val(batch, tokenizer, IGNORED_PAD_IDX=IgnoredPadIdx):\n","    texts = [TaskPrefix + data['cat'] + \":\" + data['label'] for data in batch]\n","    fids = [data['fid'] for data in batch]\n","    cats = [data['cat'] for data in batch]\n","    sposs = [data['spos'] for data in batch]\n","    eposs = [data['epos'] for data in batch]\n","    labels = [data['label'] for data in batch]\n","    encoded_seq = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=MaxLen, return_tensors=\"pt\")\n","    return encoded_seq['input_ids'], encoded_seq['attention_mask'], fids, cats, sposs, eposs, labels"],"metadata":{"id":"Zb2wJOka058f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ValBatchSize = 1\n","val_dataloader = DataLoader(val_data,\n","                            batch_size=ValBatchSize,\n","                            collate_fn=lambda batch: collate_batch_with_prompt_template_val(batch, tokenizer),\n","                            shuffle=False)"],"metadata":{"id":"Wsv8hGRr1H7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generation\n"],"metadata":{"id":"W_4L7n36s7Gf"}},{"cell_type":"code","source":["\"\"\"model reload\"\"\"\n","name = \"1701339576_flant5_v1_base_task2\"\n","time = name[:10]\n","model_name = f\"drive/MyDrive/AIcup/model_hpw/{name}/best_{time}.pt\"\n","prediction_path = val_path.replace(answer_name, answer_name.replace(\".txt\", \"_norm.txt\"))\n","model.load_state_dict(torch.load(model_name))\n","model = model.to(device)\n","\n","print(answer_path)\n","print(prediction_path)"],"metadata":{"id":"lnqc8gvssB77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(os.path.join(prediction_path), 'w', encoding='utf8') as f_predictions:\n","  for data in tqdm(val_data):\n","    fid = data['fid']\n","    cat = data['cat']\n","    spo = data['start_pos']\n","    epo = data['end_pos']\n","    content = data['content']\n","\n","    if cat and content:\n","      texts = TaskPrefix + cat + \":\" + content\n","    else:\n","      print(\"[ERROR] No cat or content\")\n","      continue\n","\n","    encoded_seq = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=MaxLen, return_tensors=\"pt\")\n","    seqs = encoded_seq['input_ids']\n","    masks = encoded_seq['attention_mask']\n","\n","    with torch.no_grad():\n","      seqs, masks = seqs.to(device), masks.to(device)\n","      if cat in NormCategory:\n","        predicted_token = model.generate(input_ids=seqs, attention_mask=masks)\n","        predicted_ids = predicted_token[0].tolist()\n","        predicted_string = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n","        # print(f'{fid}\\t{cat}\\t{spo}\\t{epo}\\t{content}\\t{predicted_string}\\n')\n","        if predicted_string:\n","          f_predictions.write(f'{fid}\\t{cat}\\t{spo}\\t{epo}\\t{content}\\t{predicted_string}\\n')\n","        else:\n","          f_predictions.write(f'{fid}\\t{cat}\\t{spo}\\t{epo}\\t{content}\\n')\n","      else:\n","        f_predictions.write(f'{fid}\\t{cat}\\t{spo}\\t{epo}\\t{content}\\n')"],"metadata":{"id":"LLKERJEZn271"},"execution_count":null,"outputs":[]}]}